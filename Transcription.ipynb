{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0659a9ca",
   "metadata": {},
   "source": [
    "# Summarizing meeting presentation along with meeting audio recording\n",
    "\n",
    "1. The Claude 3 family of models comes with new vision capabilities that allow Claude to understand and analyze images, opening up exciting possibilities for multimodal interaction\n",
    "2. Amazon Transcribe is a fully managed, automatic speech recognition (ASR) service that makes it easy for developers to add speech to text capabilities to their applications.\n",
    "3. With speaker diarization, you can distinguish between different speakers in your transcription output. Amazon Transcribe can differentiate between a maximum of 30 unique speakers and labels the text from each unique speaker with a unique value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061d0a7",
   "metadata": {},
   "source": [
    "# Convert pdf document in to images\n",
    "\n",
    "Using python library PyMuPDF, PDF document is split in to png images, one for each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd90790",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF==1.24.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.24.5)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from PyMuPDF==1.24.5) (1.24.3)\n",
      "Collecting botocore==1.34.131\n",
      "  Using cached botocore-1.34.131-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore==1.34.131) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore==1.34.131) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore==1.34.131) (2.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.34.131) (1.16.0)\n",
      "Using cached botocore-1.34.131-py3-none-any.whl (12.3 MB)\n",
      "Installing collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.162\n",
      "    Uninstalling botocore-1.34.162:\n",
      "      Successfully uninstalled botocore-1.34.162\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.34.3 requires botocore==1.35.3, but you have botocore 1.34.131 which is incompatible.\n",
      "boto3 1.34.162 requires botocore<1.35.0,>=1.34.162, but you have botocore 1.34.131 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed botocore-1.34.131\n",
      "Requirement already satisfied: langchain_aws in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.1.17)\n",
      "Requirement already satisfied: boto3<1.35.0,>=1.34.131 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain_aws) (1.34.162)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2.33 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain_aws) (0.2.37)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain_aws) (1.26.4)\n",
      "Collecting botocore<1.35.0,>=1.34.162 (from boto3<1.35.0,>=1.34.131->langchain_aws)\n",
      "  Using cached botocore-1.34.162-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<1.35.0,>=1.34.131->langchain_aws) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<1.35.0,>=1.34.131->langchain_aws) (0.10.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.33->langchain_aws) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.33->langchain_aws) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.33->langchain_aws) (0.1.108)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.33->langchain_aws) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.33->langchain_aws) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.33->langchain_aws) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.33->langchain_aws) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.162->boto3<1.35.0,>=1.34.131->langchain_aws) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.162->boto3<1.35.0,>=1.34.131->langchain_aws) (2.2.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.33->langchain_aws) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain_aws) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain_aws) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain_aws) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.33->langchain_aws) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.33->langchain_aws) (2.20.1)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain_aws) (4.4.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain_aws) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain_aws) (1.0.5)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain_aws) (3.7)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain_aws) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain_aws) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.162->boto3<1.35.0,>=1.34.131->langchain_aws) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain_aws) (3.3.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain_aws) (1.2.2)\n",
      "Using cached botocore-1.34.162-py3-none-any.whl (12.5 MB)\n",
      "Installing collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.131\n",
      "    Uninstalling botocore-1.34.131:\n",
      "      Successfully uninstalled botocore-1.34.131\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.34.3 requires botocore==1.35.3, but you have botocore 1.34.162 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed botocore-1.34.162\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF==1.24.5\n",
    "!pip install botocore==1.34.131\n",
    "!pip install langchain_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5119df11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using modelId: anthropic.claude-3-haiku-20240307-v1:0\n",
      "Using region:  ap-south-1\n"
     ]
    }
   ],
   "source": [
    "from utils.audioutil import pdf_to_pngs\n",
    "from utils.audioutil import build_slides_narration_prompt\n",
    "from utils.audioutil import get_completion\n",
    "from utils.audioutil import write_string_to_s3\n",
    "from utils.audioutil import write_string_array_to_s3\n",
    "from utils.audioutil import audio_transcribe\n",
    "from utils.audioutil import get_llm\n",
    "from utils.audioutil import convert_transctiption_to_txt_file\n",
    "from utils.audioutil import build_previous_slides_prompt_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc44806",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'audio-ppt-summarization-demo'\n",
    "input_file_key = 'input/Sagemaker_launch.pdf' \n",
    "output_file_key = 'output/slide_narration.txt' \n",
    "output_array_key = 'output/slide_narration_array.txt' \n",
    "output_file_key_audio = 'output/audio.txt' \n",
    "output_file_key_summary = 'output/summary.txt' \n",
    "output_file_key_summary_langchain = 'output/summary_langchain.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "079b0456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create output directory for images\n",
    "imagedir = \"images/slides\"\n",
    "if not os.path.exists(\"images/slides\"):\n",
    "    os.makedirs(\"images/slides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4eafc3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genarating Images\n",
      "images/slides/page_1.png\n",
      "images/slides/page_2.png\n",
      "images/slides/page_3.png\n",
      "images/slides/page_4.png\n",
      "images/slides/page_5.png\n",
      "images/slides/page_6.png\n",
      "images/slides/page_7.png\n",
      "images/slides/page_8.png\n",
      "images/slides/page_9.png\n",
      "images/slides/page_10.png\n",
      "images/slides/page_11.png\n",
      "images/slides/page_12.png\n",
      "images/slides/page_13.png\n",
      "images/slides/page_14.png\n",
      "images/slides/page_15.png\n",
      "images/slides/page_16.png\n",
      "images/slides/page_17.png\n",
      "images/slides/page_18.png\n",
      "images/slides/page_19.png\n",
      "images/slides/page_20.png\n",
      "images/slides/page_21.png\n",
      "images/slides/page_22.png\n",
      "images/slides/page_23.png\n",
      "images/slides/page_24.png\n",
      "images/slides/page_25.png\n",
      "images/slides/page_26.png\n",
      "images/slides/page_27.png\n",
      "images/slides/page_28.png\n",
      "images/slides/page_29.png\n",
      "images/slides/page_30.png\n",
      "images/slides/page_31.png\n",
      "images/slides/page_32.png\n",
      "images/slides/page_33.png\n",
      "images/slides/page_34.png\n",
      "images/slides/page_35.png\n",
      "images/slides/page_36.png\n",
      "images/slides/page_37.png\n",
      "images/slides/page_38.png\n",
      "images/slides/page_39.png\n",
      "images/slides/page_40.png\n",
      "images/slides/page_41.png\n",
      "images/slides/page_42.png\n",
      "images/slides/page_43.png\n",
      "images/slides/page_44.png\n",
      "Image genaration done\n"
     ]
    }
   ],
   "source": [
    "# Call the pdf_to_pngs function to convert the PDF to PNG images\n",
    "print('Genarating Images')\n",
    "pdf_pngs = pdf_to_pngs(bucket_name, input_file_key )\n",
    "print('Image genaration done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a4808",
   "metadata": {},
   "source": [
    "# Convert image into slide narration\n",
    "\n",
    "The best way to pass Claude charts and graphs is to take advantage of its vision capabilities. That is, give Claude an image of the chart or graph, along with a text question about it. \n",
    "We will use claude-haiku to generate narration for the slides. To improve the accuracy, we will also be passing the previous three slide narrations to claude-haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fed72c41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:05,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:09,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:13,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:16,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:21,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:25,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:30,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:33,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:36,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:42,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:47,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:54,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:58,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [01:03,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [01:08,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [01:13,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [01:17,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [01:22,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [01:27,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [01:29,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [01:32,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [01:36,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [01:39,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [01:43,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [01:47,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [01:51,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [01:56,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [01:59,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [02:03,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [02:08,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [02:13,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [02:19,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [02:24,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [02:29,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [02:34,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [02:39,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [02:44,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [02:49,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [02:52,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [02:58,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [03:02,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [03:04,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building narrative for : Slide 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [03:07,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining narrations\n",
      "Generation of slide narration done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we use our functions to narrate the entire deck. Note that this may take a few minutes to run (often up to 10).\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "previous_slide_narratives = []\n",
    "for i, pdf_png in tqdm(enumerate(pdf_pngs)):\n",
    "    print('Building narrative for : Slide ' + str(i))\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": 'user',\n",
    "            \"content\": [\n",
    "                {\"text\": build_slides_narration_prompt(previous_slide_narratives)},\n",
    "                {\"image\": {\n",
    "                    \"format\": 'jpeg',\n",
    "                    \"source\": {\"bytes\": pdf_png }\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    completion = get_completion(messages)\n",
    "\n",
    "    pattern = r\"<narration>(.*?)</narration>\"\n",
    "    match = re.search(pattern, completion.strip(), re.DOTALL)\n",
    "    if match:\n",
    "        narration = match.group(1)\n",
    "    else:\n",
    "        raise ValueError(\"No narration available.\")\n",
    "    \n",
    "    previous_slide_narratives.append(narration)\n",
    "    # If you want to see the narration we produced, uncomment the below line\n",
    "    # print(narration)\n",
    "print('Combining narrations')\n",
    "slide_narration = build_previous_slides_prompt_full(previous_slide_narratives)\n",
    "print(\"Generation of slide narration done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6712e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store narration array in s3 bucket \n",
    "write_string_array_to_s3(bucket_name, output_array_key, previous_slide_narratives)\n",
    "# Store narration in S3:\n",
    "write_string_to_s3(bucket_name, output_file_key, slide_narration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b6b9c",
   "metadata": {},
   "source": [
    "# Meeting recording audio to text\n",
    "\n",
    "Amazon Transcribe will generate text from the meeting recording file and also identify speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e26278dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Calling transcribe-----\n",
      "sagemaker already exists.\n",
      "Do you want to override the existing job (Y/N): y\n",
      "Audio transcription done\n"
     ]
    }
   ],
   "source": [
    "# Meeting Audio transcription using transcribe\n",
    "print('--Calling transcribe-----');\n",
    "result = audio_transcribe(bucket_name, audio_file_key = 'input/sagemaker.m4a',  max_speakers = 10)\n",
    "print(\"Audio transcription done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dca2557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Transcription result json in s3 bucket \n",
    "transcript_content, transcript_output_file = convert_transctiption_to_txt_file(bucket_name, 'output/Transcription.json')\n",
    "\n",
    "# Store Transcription text in S3:\n",
    "write_string_to_s3(bucket_name, output_file_key_audio, transcript_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b3fc8",
   "metadata": {},
   "source": [
    "# Summarize\n",
    "\n",
    "Finally claude-sonnet is used to summarize the presennation along with the meeting reording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fd7143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling LLM\n",
      "Here is a summary of the call transcript and presentation:\n",
      "\n",
      "The presentation introduces Amazon SageMaker Studio, a new integrated development environment (IDE) for machine learning on AWS. It begins by discussing the importance of machine learning and the challenges companies face in adopting it, such as implementing ML workflows, data preparation, model training, deployment, monitoring, and collaboration. \n",
      "\n",
      "Amazon SageMaker was created to help customers overcome these hurdles. SageMaker Studio brings together several new capabilities into a unified web interface to accelerate the ML workflow from data to deployed model. These include collaborative notebooks, experiment tracking, debugging tools, automated model building, and monitoring of live models.\n",
      "\n",
      "The presenters showcase how SageMaker Studio's visual UI allows data scientists to easily set up experiments, track iterations, compare results, debug issues, and deploy models - all within the same environment. They highlight how the IDE facilitates collaboration by enabling sharing of notebooks and results.\n",
      "\n",
      "A demo walks through the key SageMaker Studio features like the experiments dashboard, debugger interface, model monitoring charts, and automated model creation with SageMaker Autopilot. The presenters emphasize how Studio improves productivity by streamlining the entire ML lifecycle.\n",
      "\n",
      "The latter part features a customer story from GE Healthcare. They discuss how GE is using AWS AI services like SageMaker to build and deploy healthcare solutions globally. This includes applications for ultrasound analysis, X-ray diagnostics, and automated brain imaging - all aimed at improving patient care and clinical outcomes.\n",
      "\n",
      "In summary, Amazon SageMaker Studio aims to provide a comprehensive, integrated environment to build, debug, deploy and monitor machine learning models on AWS with greater speed and efficiency.\n",
      "\n",
      "Key action items and follow-ups:\n",
      "\n",
      "- Attend related breakout sessions on SageMaker Notebooks, Debugger, Autopilot, and Model Monitor\n",
      "- Explore AWS Machine Learning University courses and certifications \n",
      "- Provide feedback on the session via the mobile app survey\n",
      "- Visit aws.training/machinelearning for ML training and certification resources\n"
     ]
    }
   ],
   "source": [
    "# Call LLM to Summarize\n",
    "print(\"Calling LLM\")\n",
    "llm = get_llm()\n",
    "\n",
    "prompt = f\"\"\"Provide a detailed summary of the following call transcript :\n",
    "            {transcript_content}\n",
    "            and presentation narrative of the powerpoint presentation which was presented during the call:\n",
    "            {slide_narration}\n",
    "            into one or more clear and \n",
    "            readable paragraphs. Generate a brief summary highlighting the main ideas, viewpoints, or statements.\n",
    "            At the end of your summary, give a bullet point list of the key action \n",
    "            items, to-do's, and followup activities\n",
    "            \"\"\"\n",
    "\n",
    "response_text = llm.invoke(prompt)\n",
    "\n",
    "#Display response\n",
    "print(response_text.content)\n",
    "\n",
    "# Store summary in S3:\n",
    "write_string_to_s3(bucket_name, output_file_key_summary, response_text.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f1bb70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
